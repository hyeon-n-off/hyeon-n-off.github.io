---
title: "모델학습법IV: 손실 함수"
date: 2024-03-03 00:00:00 +09:00
categories: [Deep Learning Basic]
tags:
  [
    .
    .
    .
  ]
---
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/f089cdbe-75e9-49a3-89c4-e3613e0024ef){: width="60%"}

이 포스팅에서는 손실 함수에 대해 다뤄볼 것이다.

<hr>

### **손실 함수의 종류**
> #### **평균제곱오차(Mean Squared Error, MSE)**
> 실제값(y)와 모델이 추정한 값(y-hat)사이의 <span style="color:#A073F1">차이를 제곱</span>하여 평균낸 값 또는 함수  <br>
>
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/cc31d2a3-cf28-4363-a1d2-a2d2db99083a){: width="70%"}
>
> - Quadratic Loss 또는 L2 Loss 라고도 불린다.
> - 실제값과 추론값 사이의 오차가 제곱으로 더해지므로 초반 학습이 빠르나 <span style="color:#A073F1">이상치(outlier)에 민감하다</span>는 특징을 가진다.

<br>

> #### **평균절대오차(Mean Absolute Error, MAE)**
> 실제값(y)와 모델이 추정한 값(y-hat)사이의 차이에 대한 절대값을 평균낸 값 또는 함수  <br>
>
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/21faae79-2003-4f03-8907-9cfba6afb523){: width="72%"}
>
> - L1 Loss 라고도 불린다.
> - 추정하고자 하는 값과 단위가 같기 때문에 직관적으로 이해하기 쉽다.
> - <span style="color:#A073F1">손실 함수의 값이 최소값에 가까워져도 미분값은 동일하기 때문에 점핑이 일어날 수 있으며 손실 함수의 크기를 직접 줄여주어야 한다.</span>

<br>

> #### **Huber Loss**
> 오차가 일정 수준 이하일 때는 MSE, 그렇지 않을 때는 MAE를 사용하여 두 손실 함수의 장점을 결합한 방법  <br>
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/e9417967-57f7-42e2-8813-8f3098d0315e){: width="73%"}

<br>

> #### **교차 엔트로피(Cross Entropy, CE)**
> 주어진 확률 변수 또는 사건 집합에 대한 <span style="color:#A073F1">두 확률 분포 간의 차이를 측정</span>하는 함수  <br>
> - 이진 분류 문제에 적용되는 교차 엔트로피를 BCE(Binary Cross Entropy)라고 부르며, 이는 Log Loss라고도 불린다.  <br>
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/67651ff8-6d57-4a2e-ac7e-795218abff0b){: width="73%"}

<br>

> #### **Hinge Loss**
> 모델이 만드는 결정 경제(decision boundary)와 데이터 사이의 마진(margin)을 최대화하는 것을 목적으로 하는 손실 함수  <br>
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/25a33277-e637-4299-aee3-6c64038d0818){: width="85%"}
> - 이진 분류 문제에서 사용하는 손실 함수로, y를 +1(pos) 또는 -1(neg)로 설정한다.
> - SVM Loss라고도 부른다.

<hr>

### **손실 함수의 해석**
**Vanishing Gradient Problem**  <br>
레이어의 수가 증가할수록 활성화 함수의 미분값(0~1)이 계속해서 곱해져, 가중치에 따른 미분값이 0에 수렴하게 되는 문제  <br>
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/f4f4d3d2-4820-403c-91d5-5cd0034f0b46)

#### **역전파(Backpropagation) 관점**
뉴런이 1개인 단순한 예를 들어보자.  <br>
<p align="center"><img src="https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/320cee25-d3b6-4d62-8cff-f2333720ff5b" width="70%"></p>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/921e637f-8a0e-47c3-922f-7ce4f5a353ed)

##### **MSE 간 파라미터를 다르게 해보자.**
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/ad7b8fad-c83c-413a-b2ef-31c975c3b9e2)

학습이 느리다는 것은 dC/dw와 dC/db가 작음을 의미한다. 그렇다면 왜 미분값이 작게 나타날까?  <br>
활성화 함수인 시그모이드 함수가 곱해지기 때문이다.  <br>

##### **동일 파라미터에서 손실 함수를 다르게 해보자.**

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/ebfe9318-a2cd-4a0f-a816-ed01485fca83){: width="75%"}

- MSE와 달리 CE는 <span style="color:#A073F1">출력 레이어</span>에서의 에러값에 활성화 함수의 미분값이 곱해지지 않아 gradient vanishing problem에서 좀 더 자유롭다. (학습이 좀 더 빨리 된다.)
- 그러나 <span style="color:#A073F1">히든 레이어</span>의 노드들에서는 활성화 함수의 미분값이 계속해서 곱해지므로 레이어가 여럿 사용될 경우에는 결국 gradient vanishing problem에서 자유로울 수 없다.
- 이러한 관점에서 활성화 함수의 미분값이 0 또는 1로만 표현되는 <span style="background-color:#fff5b1">ReLU</span>는 훌륭한 선택이지다.

#### **최대가능우도(Maximum likehood) 관점**
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/648c3bd6-b677-4526-a46d-d2e221724a11)

<br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/2b30afb8-07b9-48ed-a4d3-0c034cd496bb)

<br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/d56dd39c-44e7-4648-91ee-48bde933f279){: width="97%"}

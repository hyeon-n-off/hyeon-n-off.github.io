---
title: "성능 고도화 방법I: 드롭아웃(Dropout), 정규화(Normalization)"
date: 2024-03-07 00:00:00 +09:00
categories: [Deep Learning Basic]
tags:
  [
    .
    .
    .
  ]
---
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/97a8d272-5e59-4b18-9632-52caf7f38e5a)

이 포스팅에서는 다양한 성능 고도화 방법 중 드롭아웃과 정규화에 대해서 다뤄보겠습니다.

<hr>

### **과적합, 편향과 분산**
#### **과적합(Overfitting)**

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/629271f8-91bc-42da-a116-13adc427bcb8){: width='50%'}
학습 데이터 오차가 일반화 오차에 비하여 현격하게 낮아지는 현상이다.  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/57d9b8b8-39d1-4228-b561-541041e28ab2)
과소적합(Underfitting)이나 과적합(Overfitting)이 아닌 상태가 적합한 상태이다. 이런 상태의 모델을 강건하다(Robust)고 표현한다.  <br>

#### **편향과 분산(Bias, Variance)**

> **편향(Bias)**
> Unseen Data에 대한 모델의 예측값의 <span style="background-color:#fff5b1">평균 에러</span>  <br>
> **분산(Variance)**
> Unseen Data에 대한 모델 예측값의 <span style="background-color:#fff5b1">분산</span>  <br>
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/80627bb6-5d70-41c1-9f93-50c6f12debf3)

모델의 복잡성이 증가하면 분산은 <span style="color:#9B111E">증가</span>, 편향은 <span style="color:#0047AB">감소</span>하는 경향이 있고,  <br>
모델의 복잡성이 감소하면 분산은 <span style="color:#0047AB">감소</span>, 편향은 <span style="color:#9B111E">증가</span>하는 경향이 있다.  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/de49684e-750e-4bcc-aa11-80004a66f47b){: width='70%'}

모델을 학습할 때는 편향과 분산 두 가지를 동시에 최소화하는 방향으로 트레이드오프(Trade-off)를 고려해야한다.  <br>

<hr>

### **드롭아웃(Dropout)**
드롭아웃(Dropout)은 모델 학습 시, 임의의 가중치 노드를 일정확률로 비활성화시키는 방법이다.  <br>
<span style="background-color:#fff5b1">①과적합을 방지하</span>고 모델의 <span style="background-color:#fff5b1">②일반화 성능을 향상</span>시키기 위한 기법이다.  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/644be37f-a6ee-498c-bfab-7fca97bbcb38)

<br>
우리의 모델은 훈련 데이터에 매우 민감하게 반응할 수 있으며, 특정 가중치에 과도하게 의존할 수 있다.  <br>
드롭아웃을 적용하여 모델이 특정 뉴런에 과도하게 의존하지 않도록 하는 것이 목적이다.  <br>

이러한 무작위 제거는 마치 여러 모델을 앙상블한 효과를 가지며 모델의 일반화 성능을 향상시킨다.  <br>

<hr>

### **정규화**
> **피처 스케일링(feature Scaling)**  <br>
> 서로 다른 입력 데이터의 값을 일정한 범위로 맞추는 작업  <br>
> <br>
> **① 모델이 특정 데이터의 편향성을 갖는 것을 방지할 수 있다.** <br>
> **② 거리 기반 알고리즘(K-Means, ...)에서의 영향 최소화**  <br>
> 거리를 기반으로 하는 알고리즘은 특성 간의 거리에 영향을 받는다. 따라서 특성 간 크기 차이가 클 경우, 거리 측정이 특정 특성에 크게 의존할 수 있다.  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/c827fc8c-1980-414a-94c1-3f8c5f4d0349)

3개의 레이어에서 활성화 함수로 시그모이드를 사용한다고 가정했을 때,  <br>
위 그림과 같이 출력이 한쪽으로 편향되어 나타나게 되는 것을 볼 수 있다.  <br>
이는 기울기 소실 문제나 기울기 폭발 문제를 야기하게 된다.  <br>

#### **여러가지 정규화 방법**
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/a4e9cdb6-3efd-443e-b484-29b4b77f50ed)

**배치 정규화**: 배치(Batch) 단위의 데이터를 기준으로 평균과 분산을 계산하여 <span style="background-color:#fff5b1">활성화 레이어 이후의 출력을 정규화</span>하는 방법  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/eba3041e-5fa1-49ee-8569-a59c32067999){: width='80%'}

기본적으로 정규화 레이어는 <span style="background-color:#fff5b1">① 완전 연결 레이어 이후</span>, <span style="background-color:#fff5b1">② 활성화 함수를 적용하기 전</span>에 적용하는 것을 추천한다.  <br>
배치 정규화의 목적이 출력을 의도한대로 나오게 하는데 있기 때문이다.  <br>
<br>

**레이어 정규화**: 개별 데이터 샘플 내에서 모든 특징들의 평균과 분산을 계산하여 정규화하는 방법  <br>
> 배치 정규화의 단점을 보완한 방법 중 하나.  <br> 
> 배치의 크기가 작거나, 데이터의 길이가 일정하지 않은 시계열 데이터에서 유용하게 사용된다.  <br>

**인스턴스 정규화**: 각 데이터 샘플의 각 채널에서 평균과 분산을 계산하여 정규화하는 방법  <br>

> 통상적으로 이미지 스타일 변환(Image Style Transfer)와 같은 분야에서 각 데이터의 고유한 정보를 유지하기 위해 사용된다.  <br>

**그룹 정규화**: 채널을 여러 그룹으로 나눈 후, 각 그룹 내의 평균과 분산을 계산하여 정규화하는 방법  <br>
> 배치 사이즈를 극복하기 위한 방법 중 하나이다.  <br>
> 특히 객체 인식(Object detection)이나 영역 분할(Segmentation)과 같이 배치 사이즈를 늘리기 어려운 상황에서 활용된다.  <br>

---
title: "성능 고도화 방법II: 가중치 초기화, 규제화, 학습률"
date: 2024-03-07 00:00:00 +09:00
categories: [Deep Learning Basic]
tags:
  [
    .
    .
    .
  ]
---
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/c7ef3d93-13e2-42a3-817b-6b81a0951a67)

이 포스팅에서는 다양한 성능 고도화 방법 중 가중치 초기화, 규제화 그리고 학습률에 대해서 다뤄보겠습니다.  <br>

<hr>

### **가중치 초기화**
모델이 생성될 때, 파라미터들의 초기값을 지정해주는 것을 가중치 초기화라고 한다.  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/6f3b7e97-b2c5-4d5c-89ca-5cc4ec25e084){: width='60%'}

가중치의 초기값에 따라 모델 학습 형태의 차이가 크게 달라지기에 가중치 초기화는 중요한 부분이다.  <br>

> **주요 방법론**  <br>
> <br>
> ① **0으로 초기화 (torch.nn.init.zeros_)**  <br>
>> 손실함수의 기울기가 존재하더라도, 가중치의 값이 0이므로 업데이터가 되지 않아 모델 학습이 불가능하다.  <br>
>
> ② **균등분포로 초기화 (torch.nn.init.uniform_)**  <br>
>> 균등하게 가중치를 초기화하면 값이 고르게 잘 퍼지지만 출력값의 범위가 넓어진다.  <br>
>> 출력값의 범위가 넓어지면 노드 간 학습 불균형이 일어난다.  <br>
>
> ③ **표준정규분포로 초기화 (torch.nn.init.normal_)**  <br>
>> 균등분포와 마찬가지로 레이어를 통과할수록 출력범위가 점점 커진다.  <br>
>
> ④ **Xavier 초기화 (torch.nn.init.xavier_normal_)**  <br>
>> <span style="background-color:#fff5b1">Sigmoid, tanh</span>와 같은 함수에서 효과적이다.  <br>
>> 노드 수에 비례하여 가중치를 초기화한다.     <br>
>
> ⑥ **He 초기화 (torch.nn.init.kaiming_normal_)**  <br>
>> <span style="background-color:#fff5b1">ReLU, Leaky ReLU</span>와 같은 함수에서 효과적이다.  <br>
>> Xavier 방법보다 가중치의 분포가 더 넓다.  <br>
> 
> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/d55b79f3-f1dd-449d-9a64-b2dc075931ae){: width='70%'}
>  <br>
> 자세한 내용은 [여기](https://alltommysworks.com/%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94/)를 참고하세요.

### **가중치 감쇠**
큰 가중치에 대한 패널티를 부과함으로써 모델의 가중치를 작게 유지하려고 한다. 이를 통해 모델의 복잡도를 감소시켜 모델의 과적합을 억제한다.  <br>
![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/a57e7b21-743f-466b-9b57-b5746e904ad8){: width='30%'}  <br>
<span style="color:#9B111E">( 만약 너가 복잡한 모델을 계속 쓰고 싶다면 이 penalty를 감수해야 할거야! )</span>  <br>

> **예시**  <br>
> [1, 1, 1, 1]  <br>
> W1: [1, 0, 0, 0]  <br>
> W2: [0.25, 0.25, 0.25, 0.25]  <br>
>  <hr>
> Linear Classification 관점에서 W1와 W2는 같다.  <br>
> <br>
> L1 Regression은 W1을 더 선호한다.  <br>
> 가중치 W에서 0의 개수에 따라 모델의 복잡도를 계산한다.  <br>
> (0이 아닌 요소들의 개수를 통해 모델의 복잡도를 판단한다.)  <br>
> <br>
> L2 Regression은 W2를 더 선호한다.  <br>
> W의 모든 요소가 영향을 줬으면 한다. 어떠한 요소에만 의존하기보다 모든 요소가 골고루 영향을 미치길 원한다.  <br>
> (요소들이 전체적으로 퍼져있을 때 덜 복잡하다고 판단한다.)  <br>

### **학습 조기 종료(Early Stopping)**
모델 학습 시 과적합을 방지해주는 방법 중 하나이다.  <br>
학습이 진행될수록, 모델의 학습 오차는 계속해서 줄어들어 과적합이 되고 검증 오차는 어느 순간 증가한다.  <br>

![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/629271f8-91bc-42da-a116-13adc427bcb8){: width='50%'}

학습 오차는 줄어들지만 검증 오차가 줄어들지 않고 늘어나는 시점에서 학습을 중지하는 방법이다.  <br>

```python
                # 최고 점수가 갱신되었을 때는 counter를 초기화
                if phase == 'val' and epoch_evaluate > best_evaluate:
                    early_stop_counter = 0
                    best_evaluate = epoch_evaluate
                    torch.save(model.state_dict(), best_model_params_path)

            # 현재 에포크 점수가 최고 점수보다 낮다면 counter +1
            if epoch_evaluate <= best_evaluate:
                early_stop_counter += 1
            
            # counter가 사용자 지정 threshold(patience)를 넘으면 조기 종료!
            if early_stop_counter >= patience:
                print("Early Stopping....\n")
                break
```
### **학습 스케줄러**
딥러닝 훈련 과정에서 사용되는 학습률을 동적으로 조절하는 역할을 한다.  <br>
적절한 학습률 스케줄링은 학습 속도를 빠르게 하고, 지역 최소값을 벗어나게 하며, 일반적으로 더 나은 성능의 모델을 얻게 도와준다.  <br>

> **①  Constant  <br>**
>> 초기에 설정한 학습률을 변경하지 않는다.
> 
> <br>
> **② Step Decay<br>**
>> 일정한 주기마다 학습률을 일정 비율로 감소한다.  <br>
>> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/cab12aef-1719-443e-9a75-cf9360665ce7)
>> ```python 
>> scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
>> ```
> 
> <br>
> **③ Exponential Decay<br>**
>> 학습률을 지수적으로 감소한다.  <br>
>> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/dae5a460-baf8-429c-a8c5-1be45be1ee01)
>> ```python 
>> scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)
>> ```
> 
> <br>
> **④ Cosine Annealing<br>**
>> 코사인 함수를 따라 학습률이 조정되도록 설정  <br>
>> ![image](https://github.com/hyeon-n-off/hyeon-n-off.github.io/assets/161814308/5cde5bda-ad63-4631-99af-f9fdf58cf5c8)
>> ```python
>> # T_max: 최대 iteration 횟수
>> # eta_min: 최소로 떨어질 수 있는 lr (default=0) 
>> scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)
>> ```
> 
> <br>
> 자세한 내용은 [여기](https://sanghyu.tistory.com/113)를 참고하세요.